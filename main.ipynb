{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import train\n",
    "from bin_packing_dataset import BinPackingDataset\n",
    "from bin_packing_model import BinPackingLSTMModel\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 23\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Num Workers: 0\n"
     ]
    }
   ],
   "source": [
    "# Algunas constantes\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "\n",
    "NUM_WORKERS = 0 # max(os.cpu_count() - 1, 1)  # número de workers para cargar los datos\n",
    "\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Num Workers: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 10000\n",
      "Container: tensor([9., 6.])\n",
      "Boxes: tensor([[2., 4.],\n",
      "        [7., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 1.],\n",
      "        [4., 1.],\n",
      "        [0., 0.]])\n",
      "Train dataset size: 8000\n",
      "Val dataset size: 2000\n"
     ]
    }
   ],
   "source": [
    "#Creacion del dataset de entrenamiento, validacion y test\n",
    "\n",
    "full_dataset = BinPackingDataset('data2')\n",
    "print('Full dataset size:', len(full_dataset))\n",
    "container_tensor, boxes_tensor = full_dataset[0]\n",
    "print('Container:', container_tensor)\n",
    "print('Boxes:', boxes_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [int(0.8*len(full_dataset)), int(0.20*len(full_dataset))])\n",
    "print('Train dataset size:', len(train_dataset))\n",
    "print('Val dataset size:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del primer contenedor: torch.Size([10000, 2])\n",
      "Tamaño de las cajas del primer contenedor: torch.Size([10000, 11, 2])\n"
     ]
    }
   ],
   "source": [
    "# Collate para manejar secuencias de diferentes longitudes\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "def custom_collate_fn_with_padding(batch):\n",
    "    \"\"\"\n",
    "    Collate function que mantiene la estructura de contenedor y agrega padding a las secuencias de cajas.\n",
    "    \n",
    "    Args:\n",
    "        batch (list): Lista de tuplas (contenedor, cajas).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (contenedores, cajas_padded, longitudes) donde:\n",
    "            - contenedores: Tensor de tamaño (batch_size, 2).\n",
    "            - cajas_padded: Tensor de tamaño (batch_size, max_len, 2) con padding.\n",
    "            - longitudes: Tensor de tamaños originales de las secuencias de cajas.\n",
    "    \"\"\"\n",
    "    containers = torch.stack([item[0] for item in batch])  # Contenedores como tensor\n",
    "    boxes = [item[1] for item in batch]  # Lista de cajas\n",
    "    \n",
    "    # Padding de las secuencias de cajas (rellenar con ceros hasta la longitud máxima en el batch)\n",
    "    boxes_padded = rnn_utils.pad_sequence(boxes, batch_first=True)\n",
    "    \n",
    "    # Longitudes originales de cada secuencia de cajas\n",
    "    lengths = torch.tensor([len(b) for b in boxes])\n",
    "    \n",
    "    return containers, boxes_padded\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 40000\n",
    "mock_loader = torch.utils.data.DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_with_padding)\n",
    "\n",
    "container, target_seq = next(iter(mock_loader))\n",
    "print('Tamaño del primer contenedor:', container.shape)\n",
    "print('Tamaño de las cajas del primer contenedor:', target_seq.shape)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn_with_padding)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_with_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AutoRegressiveBinPackingModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, max_dim, n_layers, dropout=0.1):\n",
    "        super(AutoRegressiveBinPackingModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.max_dim = max_dim\n",
    "        \n",
    "        # Embedding para entrada (contenedor y cajas)\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # LSTM para modelar secuencias\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Proyección para obtener logits de distribuciones de width y height\n",
    "        self.fc_width = nn.Linear(hidden_dim, max_dim + 1)  # +1 para incluir el token EOS\n",
    "        self.fc_height = nn.Linear(hidden_dim, max_dim + 1)\n",
    "    \n",
    "    def forward(self, container, target_seq=None, seq_len=100, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            container: Tensor con las dimensiones del contenedor (batch_size, input_dim).\n",
    "            target_seq: Secuencia objetivo durante entrenamiento (batch_size, seq_len, input_dim).\n",
    "            seq_len: Longitud máxima de secuencia durante generación.\n",
    "            teacher_forcing_ratio: Probabilidad de usar teacher forcing (0.0 a 1.0).\n",
    "        \n",
    "        Returns:\n",
    "            Logits para distribuciones de width y height.\n",
    "        \"\"\"\n",
    "        container_emb = self.embedding(container).unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Para almacenar las salidas durante la generación\n",
    "        outputs_width = []\n",
    "        outputs_height = []\n",
    "        \n",
    "        # Estado inicial\n",
    "        generated_seq = container_emb\n",
    "        hidden = None\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            output, hidden = self.lstm(generated_seq, hidden)  # (batch_size, 1, hidden_dim)\n",
    "            \n",
    "            # Logits para width y height\n",
    "            logits_width = self.fc_width(output[:, -1, :])  # (batch_size, max_dim+1)\n",
    "            logits_height = self.fc_height(output[:, -1, :])  # (batch_size, max_dim+1)\n",
    "            outputs_width.append(logits_width)\n",
    "            outputs_height.append(logits_height)\n",
    "            \n",
    "            if target_seq is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                # Usar la secuencia objetivo (teacher forcing)\n",
    "                next_box = target_seq[:, t, :]  # (batch_size, 2)\n",
    "            else:\n",
    "                # Sampleo de la predicción\n",
    "                prob_width = F.softmax(logits_width, dim=-1)  # (batch_size, max_dim+1)\n",
    "                prob_height = F.softmax(logits_height, dim=-1)  # (batch_size, max_dim+1)\n",
    "                next_width = torch.multinomial(prob_width, num_samples=1)  # (batch_size, 1)\n",
    "                next_height = torch.multinomial(prob_height, num_samples=1)  # (batch_size, 1)\n",
    "                next_box = torch.cat([next_width, next_height], dim=1)  # (batch_size, 2)\n",
    "            \n",
    "            # Preparar la entrada para el siguiente paso\n",
    "            next_box_emb = self.embedding(next_box.float())  # Convertir a embedding\n",
    "            # Concatenamos la secuencia generada con la nueva caja\n",
    "            generated_seq = torch.cat([generated_seq, next_box_emb.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        # Apilar las salidas\n",
    "        logits_width = torch.stack(outputs_width, dim=1)  # (batch_size, seq_len, max_dim+1)\n",
    "        logits_height = torch.stack(outputs_height, dim=1)  # (batch_size, seq_len, max_dim+1)\n",
    "        return logits_width, logits_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Train Loss: 6.229360580444336\n",
      "Epoch 1/500, Val Loss: 6.197573184967041\n",
      "Epoch 2/500, Train Loss: 6.174419403076172\n",
      "Epoch 2/500, Val Loss: 6.142951965332031\n",
      "Epoch 3/500, Train Loss: 6.13849401473999\n",
      "Epoch 3/500, Val Loss: 6.09564208984375\n",
      "Epoch 4/500, Train Loss: 6.109881401062012\n",
      "Epoch 4/500, Val Loss: 6.032200813293457\n",
      "Epoch 5/500, Train Loss: 6.045889854431152\n",
      "Epoch 5/500, Val Loss: 5.986453056335449\n",
      "Epoch 6/500, Train Loss: 5.998867511749268\n",
      "Epoch 6/500, Val Loss: 5.943309783935547\n",
      "Epoch 7/500, Train Loss: 5.9423441886901855\n",
      "Epoch 7/500, Val Loss: 5.890051364898682\n",
      "Epoch 8/500, Train Loss: 5.892148971557617\n",
      "Epoch 8/500, Val Loss: 5.862393379211426\n",
      "Epoch 9/500, Train Loss: 5.832493305206299\n",
      "Epoch 9/500, Val Loss: 5.788456916809082\n",
      "Epoch 10/500, Train Loss: 5.805131912231445\n",
      "Epoch 10/500, Val Loss: 5.752199172973633\n",
      "Epoch 11/500, Train Loss: 5.7267560958862305\n",
      "Epoch 11/500, Val Loss: 5.705105781555176\n",
      "Epoch 12/500, Train Loss: 5.667925834655762\n",
      "Epoch 12/500, Val Loss: 5.6226959228515625\n",
      "Epoch 13/500, Train Loss: 5.646454811096191\n",
      "Epoch 13/500, Val Loss: 5.610865592956543\n",
      "Epoch 14/500, Train Loss: 5.59257698059082\n",
      "Epoch 14/500, Val Loss: 5.516077995300293\n",
      "Epoch 15/500, Train Loss: 5.514976501464844\n",
      "Epoch 15/500, Val Loss: 5.440334320068359\n",
      "Epoch 16/500, Train Loss: 5.458736419677734\n",
      "Epoch 16/500, Val Loss: 5.41337776184082\n",
      "Epoch 17/500, Train Loss: 5.414656162261963\n",
      "Epoch 17/500, Val Loss: 5.352751731872559\n",
      "Epoch 18/500, Train Loss: 5.330911636352539\n",
      "Epoch 18/500, Val Loss: 5.3026533126831055\n",
      "Epoch 19/500, Train Loss: 5.308465957641602\n",
      "Epoch 19/500, Val Loss: 5.281179428100586\n",
      "Epoch 20/500, Train Loss: 5.264021873474121\n",
      "Epoch 20/500, Val Loss: 5.200611114501953\n",
      "Epoch 21/500, Train Loss: 5.1785759925842285\n",
      "Epoch 21/500, Val Loss: 5.16996431350708\n",
      "Epoch 22/500, Train Loss: 5.131319999694824\n",
      "Epoch 22/500, Val Loss: 5.141780853271484\n",
      "Epoch 23/500, Train Loss: 5.103569030761719\n",
      "Epoch 23/500, Val Loss: 5.045282363891602\n",
      "Epoch 24/500, Train Loss: 5.041021347045898\n",
      "Epoch 24/500, Val Loss: 4.995616912841797\n",
      "Epoch 25/500, Train Loss: 5.032243728637695\n",
      "Epoch 25/500, Val Loss: 4.954653739929199\n",
      "Epoch 26/500, Train Loss: 4.949739456176758\n",
      "Epoch 26/500, Val Loss: 4.945268630981445\n",
      "Epoch 27/500, Train Loss: 4.965080261230469\n",
      "Epoch 27/500, Val Loss: 4.865580081939697\n",
      "Epoch 28/500, Train Loss: 4.861766815185547\n",
      "Epoch 28/500, Val Loss: 4.86482048034668\n",
      "Epoch 29/500, Train Loss: 4.820836544036865\n",
      "Epoch 29/500, Val Loss: 4.78208065032959\n",
      "Epoch 30/500, Train Loss: 4.816596984863281\n",
      "Epoch 30/500, Val Loss: 4.740729331970215\n",
      "Epoch 31/500, Train Loss: 4.73399543762207\n",
      "Epoch 31/500, Val Loss: 4.767561912536621\n",
      "Epoch 32/500, Train Loss: 4.76422643661499\n",
      "Epoch 32/500, Val Loss: 4.660281658172607\n",
      "Epoch 33/500, Train Loss: 4.65399169921875\n",
      "Epoch 33/500, Val Loss: 4.652096271514893\n",
      "Epoch 34/500, Train Loss: 4.618108749389648\n",
      "Epoch 34/500, Val Loss: 4.585366725921631\n",
      "Epoch 35/500, Train Loss: 4.580022811889648\n",
      "Epoch 35/500, Val Loss: 4.549939155578613\n",
      "Epoch 36/500, Train Loss: 4.546674728393555\n",
      "Epoch 36/500, Val Loss: 4.515725135803223\n",
      "Epoch 37/500, Train Loss: 4.5116472244262695\n",
      "Epoch 37/500, Val Loss: 4.528090476989746\n",
      "Epoch 38/500, Train Loss: 4.477766036987305\n",
      "Epoch 38/500, Val Loss: 4.448348045349121\n",
      "Epoch 39/500, Train Loss: 4.445003509521484\n",
      "Epoch 39/500, Val Loss: 4.419210433959961\n",
      "Epoch 40/500, Train Loss: 4.413643836975098\n",
      "Epoch 40/500, Val Loss: 4.411639213562012\n",
      "Epoch 41/500, Train Loss: 4.383212566375732\n",
      "Epoch 41/500, Val Loss: 4.359356880187988\n",
      "Epoch 42/500, Train Loss: 4.354971408843994\n",
      "Epoch 42/500, Val Loss: 4.329063415527344\n",
      "Epoch 43/500, Train Loss: 4.326074600219727\n",
      "Epoch 43/500, Val Loss: 4.299673080444336\n",
      "Epoch 44/500, Train Loss: 4.325508117675781\n",
      "Epoch 44/500, Val Loss: 4.2967376708984375\n",
      "Epoch 45/500, Train Loss: 4.26835823059082\n",
      "Epoch 45/500, Val Loss: 4.246296405792236\n",
      "Epoch 46/500, Train Loss: 4.242291450500488\n",
      "Epoch 46/500, Val Loss: 4.222512722015381\n",
      "Epoch 47/500, Train Loss: 4.21579647064209\n",
      "Epoch 47/500, Val Loss: 4.195219993591309\n",
      "Epoch 48/500, Train Loss: 4.190560817718506\n",
      "Epoch 48/500, Val Loss: 4.169712543487549\n",
      "Epoch 49/500, Train Loss: 4.164567947387695\n",
      "Epoch 49/500, Val Loss: 4.145973205566406\n",
      "Epoch 50/500, Train Loss: 4.142213344573975\n",
      "Epoch 50/500, Val Loss: 4.122190475463867\n",
      "Epoch 51/500, Train Loss: 4.147243499755859\n",
      "Epoch 51/500, Val Loss: 4.1005144119262695\n",
      "Epoch 52/500, Train Loss: 4.097930431365967\n",
      "Epoch 52/500, Val Loss: 4.106805801391602\n",
      "Epoch 53/500, Train Loss: 4.130959510803223\n",
      "Epoch 53/500, Val Loss: 4.070840835571289\n",
      "Epoch 54/500, Train Loss: 4.057225704193115\n",
      "Epoch 54/500, Val Loss: 4.039965629577637\n",
      "Epoch 55/500, Train Loss: 4.037587642669678\n",
      "Epoch 55/500, Val Loss: 4.0459794998168945\n",
      "Epoch 56/500, Train Loss: 4.016969680786133\n",
      "Epoch 56/500, Val Loss: 4.00269889831543\n",
      "Epoch 57/500, Train Loss: 3.998966693878174\n",
      "Epoch 57/500, Val Loss: 3.9852538108825684\n",
      "Epoch 58/500, Train Loss: 3.9804792404174805\n",
      "Epoch 58/500, Val Loss: 3.9679160118103027\n",
      "Epoch 59/500, Train Loss: 3.964954376220703\n",
      "Epoch 59/500, Val Loss: 3.949115037918091\n",
      "Epoch 60/500, Train Loss: 3.9674062728881836\n",
      "Epoch 60/500, Val Loss: 3.9322054386138916\n",
      "Epoch 61/500, Train Loss: 3.929044246673584\n",
      "Epoch 61/500, Val Loss: 3.915839672088623\n",
      "Epoch 62/500, Train Loss: 3.912755012512207\n",
      "Epoch 62/500, Val Loss: 3.900637626647949\n",
      "Epoch 63/500, Train Loss: 3.8951334953308105\n",
      "Epoch 63/500, Val Loss: 3.8836517333984375\n",
      "Epoch 64/500, Train Loss: 3.87984561920166\n",
      "Epoch 64/500, Val Loss: 3.868997573852539\n",
      "Epoch 65/500, Train Loss: 3.8662428855895996\n",
      "Epoch 65/500, Val Loss: 3.853915214538574\n",
      "Epoch 66/500, Train Loss: 3.8539490699768066\n",
      "Epoch 66/500, Val Loss: 3.841087579727173\n",
      "Epoch 67/500, Train Loss: 3.8353238105773926\n",
      "Epoch 67/500, Val Loss: 3.8308944702148438\n",
      "Epoch 68/500, Train Loss: 3.822045087814331\n",
      "Epoch 68/500, Val Loss: 3.8150222301483154\n",
      "Epoch 69/500, Train Loss: 3.809720754623413\n",
      "Epoch 69/500, Val Loss: 3.7982540130615234\n",
      "Epoch 70/500, Train Loss: 3.796093702316284\n",
      "Epoch 70/500, Val Loss: 3.7891690731048584\n",
      "Epoch 71/500, Train Loss: 3.782554864883423\n",
      "Epoch 71/500, Val Loss: 3.773963451385498\n",
      "Epoch 72/500, Train Loss: 3.771803617477417\n",
      "Epoch 72/500, Val Loss: 3.7636067867279053\n",
      "Epoch 73/500, Train Loss: 3.7576441764831543\n",
      "Epoch 73/500, Val Loss: 3.7504987716674805\n",
      "Epoch 74/500, Train Loss: 3.747628688812256\n",
      "Epoch 74/500, Val Loss: 3.7387571334838867\n",
      "Epoch 75/500, Train Loss: 3.735381603240967\n",
      "Epoch 75/500, Val Loss: 3.723029136657715\n",
      "Epoch 76/500, Train Loss: 3.7238571643829346\n",
      "Epoch 76/500, Val Loss: 3.7130630016326904\n",
      "Epoch 77/500, Train Loss: 3.71155047416687\n",
      "Epoch 77/500, Val Loss: 3.7010815143585205\n",
      "Epoch 78/500, Train Loss: 3.697216510772705\n",
      "Epoch 78/500, Val Loss: 3.690115451812744\n",
      "Epoch 79/500, Train Loss: 3.6864681243896484\n",
      "Epoch 79/500, Val Loss: 3.6777772903442383\n",
      "Epoch 80/500, Train Loss: 3.674644947052002\n",
      "Epoch 80/500, Val Loss: 3.666675090789795\n",
      "Epoch 81/500, Train Loss: 3.661207675933838\n",
      "Epoch 81/500, Val Loss: 3.6559524536132812\n",
      "Epoch 82/500, Train Loss: 3.6496310234069824\n",
      "Epoch 82/500, Val Loss: 3.637732982635498\n",
      "Epoch 83/500, Train Loss: 3.6362979412078857\n",
      "Epoch 83/500, Val Loss: 3.624450445175171\n",
      "Epoch 84/500, Train Loss: 3.6231236457824707\n",
      "Epoch 84/500, Val Loss: 3.613814353942871\n",
      "Epoch 85/500, Train Loss: 3.6110899448394775\n",
      "Epoch 85/500, Val Loss: 3.61116623878479\n",
      "Epoch 86/500, Train Loss: 3.6027119159698486\n",
      "Epoch 86/500, Val Loss: 3.5925865173339844\n",
      "Epoch 87/500, Train Loss: 3.589139938354492\n",
      "Epoch 87/500, Val Loss: 3.5798089504241943\n",
      "Epoch 88/500, Train Loss: 3.578296661376953\n",
      "Epoch 88/500, Val Loss: 3.570181131362915\n",
      "Epoch 89/500, Train Loss: 3.563744068145752\n",
      "Epoch 89/500, Val Loss: 3.559783697128296\n",
      "Epoch 90/500, Train Loss: 3.554717540740967\n",
      "Epoch 90/500, Val Loss: 3.54541015625\n",
      "Epoch 91/500, Train Loss: 3.5420265197753906\n",
      "Epoch 91/500, Val Loss: 3.5391812324523926\n",
      "Epoch 92/500, Train Loss: 3.532355785369873\n",
      "Epoch 92/500, Val Loss: 3.522432804107666\n",
      "Epoch 93/500, Train Loss: 3.519524574279785\n",
      "Epoch 93/500, Val Loss: 3.5108091831207275\n",
      "Epoch 94/500, Train Loss: 3.5078957080841064\n",
      "Epoch 94/500, Val Loss: 3.501096248626709\n",
      "Epoch 95/500, Train Loss: 3.499636173248291\n",
      "Epoch 95/500, Val Loss: 3.4894447326660156\n",
      "Epoch 96/500, Train Loss: 3.489642381668091\n",
      "Epoch 96/500, Val Loss: 3.4743871688842773\n",
      "Epoch 97/500, Train Loss: 3.4766812324523926\n",
      "Epoch 97/500, Val Loss: 3.470621109008789\n",
      "Epoch 98/500, Train Loss: 3.464221954345703\n",
      "Epoch 98/500, Val Loss: 3.453278064727783\n",
      "Epoch 99/500, Train Loss: 3.4529497623443604\n",
      "Epoch 99/500, Val Loss: 3.448685884475708\n",
      "Epoch 100/500, Train Loss: 3.443483352661133\n",
      "Epoch 100/500, Val Loss: 3.435370922088623\n",
      "Epoch 101/500, Train Loss: 3.4319047927856445\n",
      "Epoch 101/500, Val Loss: 3.423102855682373\n",
      "Epoch 102/500, Train Loss: 3.419835090637207\n",
      "Epoch 102/500, Val Loss: 3.414334774017334\n",
      "Epoch 103/500, Train Loss: 3.4113919734954834\n",
      "Epoch 103/500, Val Loss: 3.4063944816589355\n",
      "Epoch 104/500, Train Loss: 3.4019322395324707\n",
      "Epoch 104/500, Val Loss: 3.392798662185669\n",
      "Epoch 105/500, Train Loss: 3.3867814540863037\n",
      "Epoch 105/500, Val Loss: 3.3796887397766113\n",
      "Epoch 106/500, Train Loss: 3.3821654319763184\n",
      "Epoch 106/500, Val Loss: 3.368143320083618\n",
      "Epoch 107/500, Train Loss: 3.3681139945983887\n",
      "Epoch 107/500, Val Loss: 3.365995407104492\n",
      "Epoch 108/500, Train Loss: 3.3583006858825684\n",
      "Epoch 108/500, Val Loss: 3.358206033706665\n",
      "Epoch 109/500, Train Loss: 3.3482210636138916\n",
      "Epoch 109/500, Val Loss: 3.345952272415161\n",
      "Epoch 110/500, Train Loss: 3.337881088256836\n",
      "Epoch 110/500, Val Loss: 3.3291192054748535\n",
      "Epoch 111/500, Train Loss: 3.3299028873443604\n",
      "Epoch 111/500, Val Loss: 3.325718879699707\n",
      "Epoch 112/500, Train Loss: 3.321042060852051\n",
      "Epoch 112/500, Val Loss: 3.31416654586792\n",
      "Epoch 113/500, Train Loss: 3.311659336090088\n",
      "Epoch 113/500, Val Loss: 3.303717613220215\n",
      "Epoch 114/500, Train Loss: 3.3033151626586914\n",
      "Epoch 114/500, Val Loss: 3.2938151359558105\n",
      "Epoch 115/500, Train Loss: 3.293076515197754\n",
      "Epoch 115/500, Val Loss: 3.290717601776123\n",
      "Epoch 116/500, Train Loss: 3.2869865894317627\n",
      "Epoch 116/500, Val Loss: 3.2738301753997803\n",
      "Epoch 117/500, Train Loss: 3.275749683380127\n",
      "Epoch 117/500, Val Loss: 3.2723388671875\n",
      "Epoch 118/500, Train Loss: 3.272899866104126\n",
      "Epoch 118/500, Val Loss: 3.265165328979492\n",
      "Epoch 119/500, Train Loss: 3.2625298500061035\n",
      "Epoch 119/500, Val Loss: 3.256577491760254\n",
      "Epoch 120/500, Train Loss: 3.255739450454712\n",
      "Epoch 120/500, Val Loss: 3.2459378242492676\n",
      "Epoch 121/500, Train Loss: 3.248471260070801\n",
      "Epoch 121/500, Val Loss: 3.24560546875\n",
      "Epoch 122/500, Train Loss: 3.2398905754089355\n",
      "Epoch 122/500, Val Loss: 3.2345821857452393\n",
      "Epoch 123/500, Train Loss: 3.232536792755127\n",
      "Epoch 123/500, Val Loss: 3.2309412956237793\n",
      "Epoch 124/500, Train Loss: 3.229916572570801\n",
      "Epoch 124/500, Val Loss: 3.2240796089172363\n",
      "Epoch 125/500, Train Loss: 3.222240924835205\n",
      "Epoch 125/500, Val Loss: 3.2267508506774902\n",
      "Epoch 126/500, Train Loss: 3.216062068939209\n",
      "Epoch 126/500, Val Loss: 3.209564208984375\n",
      "Epoch 127/500, Train Loss: 3.210824728012085\n",
      "Epoch 127/500, Val Loss: 3.20784854888916\n",
      "Epoch 128/500, Train Loss: 3.2040305137634277\n",
      "Epoch 128/500, Val Loss: 3.2026472091674805\n",
      "Epoch 129/500, Train Loss: 3.196101665496826\n",
      "Epoch 129/500, Val Loss: 3.1958112716674805\n",
      "Epoch 130/500, Train Loss: 3.192256450653076\n",
      "Epoch 130/500, Val Loss: 3.1819286346435547\n",
      "Epoch 131/500, Train Loss: 3.1839656829833984\n",
      "Epoch 131/500, Val Loss: 3.17933988571167\n",
      "Epoch 132/500, Train Loss: 3.1808972358703613\n",
      "Epoch 132/500, Val Loss: 3.1765360832214355\n",
      "Epoch 133/500, Train Loss: 3.1742801666259766\n",
      "Epoch 133/500, Val Loss: 3.1693339347839355\n",
      "Epoch 134/500, Train Loss: 3.1661717891693115\n",
      "Epoch 134/500, Val Loss: 3.1648097038269043\n",
      "Epoch 135/500, Train Loss: 3.1605224609375\n",
      "Epoch 135/500, Val Loss: 3.1573712825775146\n",
      "Epoch 136/500, Train Loss: 3.1564903259277344\n",
      "Epoch 136/500, Val Loss: 3.1531224250793457\n",
      "Epoch 137/500, Train Loss: 3.151968002319336\n",
      "Epoch 137/500, Val Loss: 3.147617816925049\n",
      "Epoch 138/500, Train Loss: 3.1441855430603027\n",
      "Epoch 138/500, Val Loss: 3.1411871910095215\n",
      "Epoch 139/500, Train Loss: 3.1385085582733154\n",
      "Epoch 139/500, Val Loss: 3.1320459842681885\n",
      "Epoch 140/500, Train Loss: 3.135988235473633\n",
      "Epoch 140/500, Val Loss: 3.136256456375122\n",
      "Epoch 141/500, Train Loss: 3.1321468353271484\n",
      "Epoch 141/500, Val Loss: 3.1241207122802734\n",
      "Epoch 142/500, Train Loss: 3.1232476234436035\n",
      "Epoch 142/500, Val Loss: 3.120413303375244\n",
      "Epoch 143/500, Train Loss: 3.119131565093994\n",
      "Epoch 143/500, Val Loss: 3.1168975830078125\n",
      "Epoch 144/500, Train Loss: 3.1165413856506348\n",
      "Epoch 144/500, Val Loss: 3.110530376434326\n",
      "Epoch 145/500, Train Loss: 3.1096839904785156\n",
      "Epoch 145/500, Val Loss: 3.1019296646118164\n",
      "Epoch 146/500, Train Loss: 3.102992057800293\n",
      "Epoch 146/500, Val Loss: 3.1027731895446777\n",
      "Epoch 147/500, Train Loss: 3.099323272705078\n",
      "Epoch 147/500, Val Loss: 3.0995755195617676\n",
      "Epoch 148/500, Train Loss: 3.095158338546753\n",
      "Epoch 148/500, Val Loss: 3.0901236534118652\n",
      "Epoch 149/500, Train Loss: 3.0899577140808105\n",
      "Epoch 149/500, Val Loss: 3.090224266052246\n",
      "Epoch 150/500, Train Loss: 3.08780574798584\n",
      "Epoch 150/500, Val Loss: 3.0841803550720215\n",
      "Epoch 151/500, Train Loss: 3.084446907043457\n",
      "Epoch 151/500, Val Loss: 3.0816376209259033\n",
      "Epoch 152/500, Train Loss: 3.0820255279541016\n",
      "Epoch 152/500, Val Loss: 3.0833921432495117\n",
      "Epoch 153/500, Train Loss: 3.0799241065979004\n",
      "Epoch 153/500, Val Loss: 3.073103427886963\n",
      "Epoch 154/500, Train Loss: 3.073190689086914\n",
      "Epoch 154/500, Val Loss: 3.067221164703369\n",
      "Epoch 155/500, Train Loss: 3.066282272338867\n",
      "Epoch 155/500, Val Loss: 3.068376064300537\n",
      "Epoch 156/500, Train Loss: 3.0644125938415527\n",
      "Epoch 156/500, Val Loss: 3.0645742416381836\n",
      "Epoch 157/500, Train Loss: 3.06268572807312\n",
      "Epoch 157/500, Val Loss: 3.0616352558135986\n",
      "Epoch 158/500, Train Loss: 3.058457851409912\n",
      "Epoch 158/500, Val Loss: 3.0556480884552\n",
      "Epoch 159/500, Train Loss: 3.054305076599121\n",
      "Epoch 159/500, Val Loss: 3.0587124824523926\n",
      "Epoch 160/500, Train Loss: 3.055201530456543\n",
      "Epoch 160/500, Val Loss: 3.056356430053711\n",
      "Epoch 161/500, Train Loss: 3.0519652366638184\n",
      "Epoch 161/500, Val Loss: 3.049616575241089\n",
      "Epoch 162/500, Train Loss: 3.0488948822021484\n",
      "Epoch 162/500, Val Loss: 3.04445743560791\n",
      "Epoch 163/500, Train Loss: 3.0418498516082764\n",
      "Epoch 163/500, Val Loss: 3.0465426445007324\n",
      "Epoch 164/500, Train Loss: 3.040924549102783\n",
      "Epoch 164/500, Val Loss: 3.0421814918518066\n",
      "Epoch 165/500, Train Loss: 3.0414958000183105\n",
      "Epoch 165/500, Val Loss: 3.0394651889801025\n",
      "Epoch 166/500, Train Loss: 3.0348949432373047\n",
      "Epoch 166/500, Val Loss: 3.036712169647217\n",
      "Epoch 167/500, Train Loss: 3.0334954261779785\n",
      "Epoch 167/500, Val Loss: 3.034944534301758\n",
      "Epoch 168/500, Train Loss: 3.03232741355896\n",
      "Epoch 168/500, Val Loss: 3.0321671962738037\n",
      "Epoch 169/500, Train Loss: 3.0284647941589355\n",
      "Epoch 169/500, Val Loss: 3.026546001434326\n",
      "Epoch 170/500, Train Loss: 3.025604009628296\n",
      "Epoch 170/500, Val Loss: 3.0253472328186035\n",
      "Epoch 171/500, Train Loss: 3.024860382080078\n",
      "Epoch 171/500, Val Loss: 3.0237131118774414\n",
      "Epoch 172/500, Train Loss: 3.0235753059387207\n",
      "Epoch 172/500, Val Loss: 3.0230417251586914\n",
      "Epoch 173/500, Train Loss: 3.0191357135772705\n",
      "Epoch 173/500, Val Loss: 3.021195650100708\n",
      "Epoch 174/500, Train Loss: 3.017061710357666\n",
      "Epoch 174/500, Val Loss: 3.019962787628174\n",
      "Epoch 175/500, Train Loss: 3.0163307189941406\n",
      "Epoch 175/500, Val Loss: 3.0186333656311035\n",
      "Epoch 176/500, Train Loss: 3.015075206756592\n",
      "Epoch 176/500, Val Loss: 3.0172839164733887\n",
      "Epoch 177/500, Train Loss: 3.015507936477661\n",
      "Epoch 177/500, Val Loss: 3.013838768005371\n",
      "Epoch 178/500, Train Loss: 3.011209011077881\n",
      "Epoch 178/500, Val Loss: 3.007056474685669\n",
      "Epoch 179/500, Train Loss: 3.007427453994751\n",
      "Epoch 179/500, Val Loss: 3.0103912353515625\n",
      "Epoch 180/500, Train Loss: 3.0049448013305664\n",
      "Epoch 180/500, Val Loss: 3.0076751708984375\n",
      "Epoch 181/500, Train Loss: 3.0053107738494873\n",
      "Epoch 181/500, Val Loss: 3.0061817169189453\n",
      "Epoch 182/500, Train Loss: 3.004608154296875\n",
      "Epoch 182/500, Val Loss: 3.0007729530334473\n",
      "Epoch 183/500, Train Loss: 3.0013654232025146\n",
      "Epoch 183/500, Val Loss: 3.00130033493042\n",
      "Epoch 184/500, Train Loss: 2.9983654022216797\n",
      "Epoch 184/500, Val Loss: 2.999924421310425\n",
      "Epoch 185/500, Train Loss: 2.997162342071533\n",
      "Epoch 185/500, Val Loss: 3.000459909439087\n",
      "Epoch 186/500, Train Loss: 2.9978065490722656\n",
      "Epoch 186/500, Val Loss: 3.0023837089538574\n",
      "Epoch 187/500, Train Loss: 2.996560573577881\n",
      "Epoch 187/500, Val Loss: 2.9959521293640137\n",
      "Epoch 188/500, Train Loss: 2.9916582107543945\n",
      "Epoch 188/500, Val Loss: 2.9966673851013184\n",
      "Epoch 189/500, Train Loss: 2.991640567779541\n",
      "Epoch 189/500, Val Loss: 2.994997024536133\n",
      "Epoch 190/500, Train Loss: 2.9938535690307617\n",
      "Epoch 190/500, Val Loss: 2.988114595413208\n",
      "Epoch 191/500, Train Loss: 2.9891250133514404\n",
      "Epoch 191/500, Val Loss: 2.990299701690674\n",
      "Epoch 192/500, Train Loss: 2.9870076179504395\n",
      "Epoch 192/500, Val Loss: 2.9885973930358887\n",
      "Epoch 193/500, Train Loss: 2.9885449409484863\n",
      "Epoch 193/500, Val Loss: 2.9878883361816406\n",
      "Epoch 194/500, Train Loss: 2.9843850135803223\n",
      "Epoch 194/500, Val Loss: 2.982471466064453\n",
      "Epoch 195/500, Train Loss: 2.9832658767700195\n",
      "Epoch 195/500, Val Loss: 2.984433174133301\n",
      "Epoch 196/500, Train Loss: 2.980869770050049\n",
      "Epoch 196/500, Val Loss: 2.983943462371826\n",
      "Epoch 197/500, Train Loss: 2.980557441711426\n",
      "Epoch 197/500, Val Loss: 2.984797954559326\n",
      "Epoch 198/500, Train Loss: 2.9821295738220215\n",
      "Epoch 198/500, Val Loss: 2.9798812866210938\n",
      "Epoch 199/500, Train Loss: 2.9773550033569336\n",
      "Epoch 199/500, Val Loss: 2.9789392948150635\n",
      "Epoch 200/500, Train Loss: 2.977433443069458\n",
      "Epoch 200/500, Val Loss: 2.9804413318634033\n",
      "Epoch 201/500, Train Loss: 2.977820873260498\n",
      "Epoch 201/500, Val Loss: 2.9750680923461914\n",
      "Epoch 202/500, Train Loss: 2.9739949703216553\n",
      "Epoch 202/500, Val Loss: 2.9750404357910156\n",
      "Epoch 203/500, Train Loss: 2.971933364868164\n",
      "Epoch 203/500, Val Loss: 2.9724879264831543\n",
      "Epoch 204/500, Train Loss: 2.9703359603881836\n",
      "Epoch 204/500, Val Loss: 2.971923828125\n",
      "Epoch 205/500, Train Loss: 2.9712469577789307\n",
      "Epoch 205/500, Val Loss: 2.972912311553955\n",
      "Epoch 206/500, Train Loss: 2.9694061279296875\n",
      "Epoch 206/500, Val Loss: 2.9685120582580566\n",
      "Epoch 207/500, Train Loss: 2.9684033393859863\n",
      "Epoch 207/500, Val Loss: 2.966691017150879\n",
      "Epoch 208/500, Train Loss: 2.9663641452789307\n",
      "Epoch 208/500, Val Loss: 2.9668145179748535\n",
      "Epoch 209/500, Train Loss: 2.9656026363372803\n",
      "Epoch 209/500, Val Loss: 2.9676499366760254\n",
      "Epoch 210/500, Train Loss: 2.9651389122009277\n",
      "Epoch 210/500, Val Loss: 2.969374656677246\n",
      "Epoch 211/500, Train Loss: 2.9641549587249756\n",
      "Epoch 211/500, Val Loss: 2.9673032760620117\n",
      "Epoch 212/500, Train Loss: 2.9639906883239746\n",
      "Epoch 212/500, Val Loss: 2.9663429260253906\n",
      "Epoch 213/500, Train Loss: 2.9662423133850098\n",
      "Epoch 213/500, Val Loss: 2.9621715545654297\n",
      "Epoch 214/500, Train Loss: 2.9615836143493652\n",
      "Epoch 214/500, Val Loss: 2.961540937423706\n",
      "Epoch 215/500, Train Loss: 2.9596242904663086\n",
      "Epoch 215/500, Val Loss: 2.959324359893799\n",
      "Epoch 216/500, Train Loss: 2.957974672317505\n",
      "Epoch 216/500, Val Loss: 2.961411476135254\n",
      "Epoch 217/500, Train Loss: 2.956638813018799\n",
      "Epoch 217/500, Val Loss: 2.958087921142578\n",
      "Epoch 218/500, Train Loss: 2.9564664363861084\n",
      "Epoch 218/500, Val Loss: 2.9582114219665527\n",
      "Epoch 219/500, Train Loss: 2.9554028511047363\n",
      "Epoch 219/500, Val Loss: 2.959749460220337\n",
      "Epoch 220/500, Train Loss: 2.955153465270996\n",
      "Epoch 220/500, Val Loss: 2.9574177265167236\n",
      "Epoch 221/500, Train Loss: 2.9549570083618164\n",
      "Epoch 221/500, Val Loss: 2.9570388793945312\n",
      "Epoch 222/500, Train Loss: 2.9543988704681396\n",
      "Epoch 222/500, Val Loss: 2.954115867614746\n",
      "Epoch 223/500, Train Loss: 2.950676441192627\n",
      "Epoch 223/500, Val Loss: 2.951962471008301\n",
      "Epoch 224/500, Train Loss: 2.950100898742676\n",
      "Epoch 224/500, Val Loss: 2.952963352203369\n",
      "Epoch 225/500, Train Loss: 2.951134204864502\n",
      "Epoch 225/500, Val Loss: 2.952009916305542\n",
      "Epoch 226/500, Train Loss: 2.950206756591797\n",
      "Epoch 226/500, Val Loss: 2.952869176864624\n",
      "Epoch 227/500, Train Loss: 2.9512758255004883\n",
      "Epoch 227/500, Val Loss: 2.9508557319641113\n",
      "Epoch 228/500, Train Loss: 2.9492101669311523\n",
      "Epoch 228/500, Val Loss: 2.9515128135681152\n",
      "Epoch 229/500, Train Loss: 2.9486019611358643\n",
      "Epoch 229/500, Val Loss: 2.9478063583374023\n",
      "Epoch 230/500, Train Loss: 2.9460577964782715\n",
      "Epoch 230/500, Val Loss: 2.945666790008545\n",
      "Epoch 231/500, Train Loss: 2.9449896812438965\n",
      "Epoch 231/500, Val Loss: 2.945279359817505\n",
      "Epoch 232/500, Train Loss: 2.943791389465332\n",
      "Epoch 232/500, Val Loss: 2.9455504417419434\n",
      "Epoch 233/500, Train Loss: 2.9446892738342285\n",
      "Epoch 233/500, Val Loss: 2.947093963623047\n",
      "Epoch 234/500, Train Loss: 2.945749521255493\n",
      "Epoch 234/500, Val Loss: 2.9440622329711914\n",
      "Epoch 235/500, Train Loss: 2.9437482357025146\n",
      "Epoch 235/500, Val Loss: 2.9440948963165283\n",
      "Epoch 236/500, Train Loss: 2.942591667175293\n",
      "Epoch 236/500, Val Loss: 2.9408457279205322\n",
      "Epoch 237/500, Train Loss: 2.939889669418335\n",
      "Epoch 237/500, Val Loss: 2.940433979034424\n",
      "Epoch 238/500, Train Loss: 2.9388155937194824\n",
      "Epoch 238/500, Val Loss: 2.939877510070801\n",
      "Epoch 239/500, Train Loss: 2.9394311904907227\n",
      "Epoch 239/500, Val Loss: 2.9410743713378906\n",
      "Epoch 240/500, Train Loss: 2.93862247467041\n",
      "Epoch 240/500, Val Loss: 2.942870616912842\n",
      "Epoch 241/500, Train Loss: 2.9409878253936768\n",
      "Epoch 241/500, Val Loss: 2.942941427230835\n",
      "Epoch 242/500, Train Loss: 2.940919876098633\n",
      "Epoch 242/500, Val Loss: 2.9446887969970703\n",
      "Epoch 243/500, Train Loss: 2.939476490020752\n",
      "Epoch 243/500, Val Loss: 2.939164161682129\n",
      "Epoch 244/500, Train Loss: 2.9350128173828125\n",
      "Epoch 244/500, Val Loss: 2.937774658203125\n",
      "Epoch 245/500, Train Loss: 2.9353628158569336\n",
      "Epoch 245/500, Val Loss: 2.9410972595214844\n",
      "Epoch 246/500, Train Loss: 2.9391236305236816\n",
      "Epoch 246/500, Val Loss: 2.9382474422454834\n",
      "Epoch 247/500, Train Loss: 2.9371752738952637\n",
      "Epoch 247/500, Val Loss: 2.935830593109131\n",
      "Epoch 248/500, Train Loss: 2.9344072341918945\n",
      "Epoch 248/500, Val Loss: 2.93399715423584\n",
      "Epoch 249/500, Train Loss: 2.9322924613952637\n",
      "Epoch 249/500, Val Loss: 2.936387300491333\n",
      "Epoch 250/500, Train Loss: 2.933568000793457\n",
      "Epoch 250/500, Val Loss: 2.9368205070495605\n",
      "Epoch 251/500, Train Loss: 2.936206102371216\n",
      "Epoch 251/500, Val Loss: 2.9318721294403076\n",
      "Epoch 252/500, Train Loss: 2.930288076400757\n",
      "Epoch 252/500, Val Loss: 2.9344472885131836\n",
      "Epoch 253/500, Train Loss: 2.932191848754883\n",
      "Epoch 253/500, Val Loss: 2.938354015350342\n",
      "Epoch 254/500, Train Loss: 2.935053586959839\n",
      "Epoch 254/500, Val Loss: 2.930311441421509\n",
      "Epoch 255/500, Train Loss: 2.929868459701538\n",
      "Epoch 255/500, Val Loss: 2.9375460147857666\n",
      "Epoch 256/500, Train Loss: 2.935861110687256\n",
      "Epoch 256/500, Val Loss: 2.9443860054016113\n",
      "Epoch 257/500, Train Loss: 2.940098285675049\n",
      "Epoch 257/500, Val Loss: 2.9320714473724365\n",
      "Epoch 258/500, Train Loss: 2.9308581352233887\n",
      "Epoch 258/500, Val Loss: 2.9510531425476074\n",
      "Epoch 259/500, Train Loss: 2.947622537612915\n",
      "Epoch 259/500, Val Loss: 2.936087131500244\n",
      "Epoch 260/500, Train Loss: 2.934225082397461\n",
      "Epoch 260/500, Val Loss: 2.9405789375305176\n",
      "Epoch 261/500, Train Loss: 2.9405834674835205\n",
      "Epoch 261/500, Val Loss: 2.928675651550293\n",
      "Epoch 262/500, Train Loss: 2.926664352416992\n",
      "Epoch 262/500, Val Loss: 2.943408727645874\n",
      "Epoch 263/500, Train Loss: 2.9425106048583984\n",
      "Epoch 263/500, Val Loss: 2.9302477836608887\n",
      "Epoch 264/500, Train Loss: 2.9265999794006348\n",
      "Epoch 264/500, Val Loss: 2.939624786376953\n",
      "Epoch 265/500, Train Loss: 2.9372153282165527\n",
      "Epoch 265/500, Val Loss: 2.925753116607666\n",
      "Epoch 266/500, Train Loss: 2.926403760910034\n",
      "Epoch 266/500, Val Loss: 2.936896324157715\n",
      "Epoch 267/500, Train Loss: 2.9361438751220703\n",
      "Epoch 267/500, Val Loss: 2.924905300140381\n",
      "Epoch 268/500, Train Loss: 2.9258337020874023\n",
      "Epoch 268/500, Val Loss: 2.9322762489318848\n",
      "Epoch 269/500, Train Loss: 2.9297938346862793\n",
      "Epoch 269/500, Val Loss: 2.9306588172912598\n",
      "Epoch 270/500, Train Loss: 2.9283266067504883\n",
      "Epoch 270/500, Val Loss: 2.922292470932007\n",
      "Epoch 271/500, Train Loss: 2.924832344055176\n",
      "Epoch 271/500, Val Loss: 2.9311861991882324\n",
      "Epoch 272/500, Train Loss: 2.9283668994903564\n",
      "Epoch 272/500, Val Loss: 2.9242429733276367\n",
      "Epoch 273/500, Train Loss: 2.921452522277832\n",
      "Epoch 273/500, Val Loss: 2.9257588386535645\n",
      "Epoch 274/500, Train Loss: 2.9252207279205322\n",
      "Epoch 274/500, Val Loss: 2.9222755432128906\n",
      "Epoch 275/500, Train Loss: 2.921835422515869\n",
      "Epoch 275/500, Val Loss: 2.9243478775024414\n",
      "Epoch 276/500, Train Loss: 2.922825813293457\n",
      "Epoch 276/500, Val Loss: 2.9248781204223633\n",
      "Epoch 277/500, Train Loss: 2.9207615852355957\n",
      "Epoch 277/500, Val Loss: 2.920463800430298\n",
      "Epoch 278/500, Train Loss: 2.9204232692718506\n",
      "Epoch 278/500, Val Loss: 2.924290657043457\n",
      "Epoch 279/500, Train Loss: 2.9210448265075684\n",
      "Epoch 279/500, Val Loss: 2.9195730686187744\n",
      "Epoch 280/500, Train Loss: 2.917189598083496\n",
      "Epoch 280/500, Val Loss: 2.9230847358703613\n",
      "Epoch 281/500, Train Loss: 2.9202444553375244\n",
      "Epoch 281/500, Val Loss: 2.920742988586426\n",
      "Epoch 282/500, Train Loss: 2.916966438293457\n",
      "Epoch 282/500, Val Loss: 2.9193098545074463\n",
      "Epoch 283/500, Train Loss: 2.9163010120391846\n",
      "Epoch 283/500, Val Loss: 2.9175682067871094\n",
      "Epoch 284/500, Train Loss: 2.9157180786132812\n",
      "Epoch 284/500, Val Loss: 2.918449878692627\n",
      "Epoch 285/500, Train Loss: 2.9175519943237305\n",
      "Epoch 285/500, Val Loss: 2.9171745777130127\n",
      "Epoch 286/500, Train Loss: 2.9144139289855957\n",
      "Epoch 286/500, Val Loss: 2.916638135910034\n",
      "Epoch 287/500, Train Loss: 2.9152026176452637\n",
      "Epoch 287/500, Val Loss: 2.9142537117004395\n",
      "Epoch 288/500, Train Loss: 2.9141860008239746\n",
      "Epoch 288/500, Val Loss: 2.91634464263916\n",
      "Epoch 289/500, Train Loss: 2.914752244949341\n",
      "Epoch 289/500, Val Loss: 2.915771722793579\n",
      "Epoch 290/500, Train Loss: 2.913498878479004\n",
      "Epoch 290/500, Val Loss: 2.915750026702881\n",
      "Epoch 291/500, Train Loss: 2.9115405082702637\n",
      "Epoch 291/500, Val Loss: 2.9167308807373047\n",
      "Epoch 292/500, Train Loss: 2.912982940673828\n",
      "Epoch 292/500, Val Loss: 2.913728713989258\n",
      "Epoch 293/500, Train Loss: 2.9121105670928955\n",
      "Epoch 293/500, Val Loss: 2.9136948585510254\n",
      "Epoch 294/500, Train Loss: 2.9127726554870605\n",
      "Epoch 294/500, Val Loss: 2.9125266075134277\n",
      "Epoch 295/500, Train Loss: 2.911285400390625\n",
      "Epoch 295/500, Val Loss: 2.913496494293213\n",
      "Epoch 296/500, Train Loss: 2.9110007286071777\n",
      "Epoch 296/500, Val Loss: 2.911609172821045\n",
      "Epoch 297/500, Train Loss: 2.9104857444763184\n",
      "Epoch 297/500, Val Loss: 2.9124741554260254\n",
      "Epoch 298/500, Train Loss: 2.909806489944458\n",
      "Epoch 298/500, Val Loss: 2.9127864837646484\n",
      "Epoch 299/500, Train Loss: 2.9103925228118896\n",
      "Epoch 299/500, Val Loss: 2.9113030433654785\n",
      "Epoch 300/500, Train Loss: 2.9092185497283936\n",
      "Epoch 300/500, Val Loss: 2.9101758003234863\n",
      "Epoch 301/500, Train Loss: 2.908938407897949\n",
      "Epoch 301/500, Val Loss: 2.9103660583496094\n",
      "Epoch 302/500, Train Loss: 2.9090468883514404\n",
      "Epoch 302/500, Val Loss: 2.910836696624756\n",
      "Epoch 303/500, Train Loss: 2.9089105129241943\n",
      "Epoch 303/500, Val Loss: 2.908979892730713\n",
      "Epoch 304/500, Train Loss: 2.90751051902771\n",
      "Epoch 304/500, Val Loss: 2.9103994369506836\n",
      "Epoch 305/500, Train Loss: 2.9096853733062744\n",
      "Epoch 305/500, Val Loss: 2.9105186462402344\n",
      "Epoch 306/500, Train Loss: 2.9086101055145264\n",
      "Epoch 306/500, Val Loss: 2.9078330993652344\n",
      "Epoch 307/500, Train Loss: 2.90653395652771\n",
      "Epoch 307/500, Val Loss: 2.9117586612701416\n",
      "Epoch 308/500, Train Loss: 2.9094159603118896\n",
      "Epoch 308/500, Val Loss: 2.9100098609924316\n",
      "Epoch 309/500, Train Loss: 2.908959150314331\n",
      "Epoch 309/500, Val Loss: 2.906458854675293\n",
      "Epoch 310/500, Train Loss: 2.906959056854248\n",
      "Epoch 310/500, Val Loss: 2.91634202003479\n",
      "Epoch 311/500, Train Loss: 2.9118456840515137\n",
      "Epoch 311/500, Val Loss: 2.914700984954834\n",
      "Epoch 312/500, Train Loss: 2.911947727203369\n",
      "Epoch 312/500, Val Loss: 2.9084935188293457\n",
      "Epoch 313/500, Train Loss: 2.9084701538085938\n",
      "Epoch 313/500, Val Loss: 2.9113059043884277\n",
      "Epoch 314/500, Train Loss: 2.9111862182617188\n",
      "Epoch 314/500, Val Loss: 2.906625270843506\n",
      "Epoch 315/500, Train Loss: 2.90409517288208\n",
      "Epoch 315/500, Val Loss: 2.9084436893463135\n",
      "Epoch 316/500, Train Loss: 2.9072890281677246\n",
      "Epoch 316/500, Val Loss: 2.905851364135742\n",
      "Epoch 317/500, Train Loss: 2.9035558700561523\n",
      "Epoch 317/500, Val Loss: 2.9108500480651855\n",
      "Epoch 318/500, Train Loss: 2.9063243865966797\n",
      "Epoch 318/500, Val Loss: 2.9060144424438477\n",
      "Epoch 319/500, Train Loss: 2.9036169052124023\n",
      "Epoch 319/500, Val Loss: 2.907689094543457\n",
      "Epoch 320/500, Train Loss: 2.905648946762085\n",
      "Epoch 320/500, Val Loss: 2.905880928039551\n",
      "Epoch 321/500, Train Loss: 2.9041543006896973\n",
      "Epoch 321/500, Val Loss: 2.9058423042297363\n",
      "Epoch 322/500, Train Loss: 2.903499126434326\n",
      "Epoch 322/500, Val Loss: 2.9043755531311035\n",
      "Epoch 323/500, Train Loss: 2.902754783630371\n",
      "Epoch 323/500, Val Loss: 2.9036753177642822\n",
      "Epoch 324/500, Train Loss: 2.9020237922668457\n",
      "Epoch 324/500, Val Loss: 2.903747797012329\n",
      "Epoch 325/500, Train Loss: 2.9028568267822266\n",
      "Epoch 325/500, Val Loss: 2.901524543762207\n",
      "Epoch 326/500, Train Loss: 2.9012346267700195\n",
      "Epoch 326/500, Val Loss: 2.902339458465576\n",
      "Epoch 327/500, Train Loss: 2.902132272720337\n",
      "Epoch 327/500, Val Loss: 2.9023680686950684\n",
      "Epoch 328/500, Train Loss: 2.90033221244812\n",
      "Epoch 328/500, Val Loss: 2.903954029083252\n",
      "Epoch 329/500, Train Loss: 2.9018263816833496\n",
      "Epoch 329/500, Val Loss: 2.9023666381835938\n",
      "Epoch 330/500, Train Loss: 2.899108409881592\n",
      "Epoch 330/500, Val Loss: 2.9026851654052734\n",
      "Epoch 331/500, Train Loss: 2.8997647762298584\n",
      "Epoch 331/500, Val Loss: 2.900378942489624\n",
      "Epoch 332/500, Train Loss: 2.8991341590881348\n",
      "Epoch 332/500, Val Loss: 2.9008703231811523\n",
      "Epoch 333/500, Train Loss: 2.8987581729888916\n",
      "Epoch 333/500, Val Loss: 2.90043306350708\n",
      "Epoch 334/500, Train Loss: 2.8989241123199463\n",
      "Epoch 334/500, Val Loss: 2.900219440460205\n",
      "Epoch 335/500, Train Loss: 2.8983802795410156\n",
      "Epoch 335/500, Val Loss: 2.8995018005371094\n",
      "Epoch 336/500, Train Loss: 2.8983259201049805\n",
      "Epoch 336/500, Val Loss: 2.9003984928131104\n",
      "Epoch 337/500, Train Loss: 2.8970718383789062\n",
      "Epoch 337/500, Val Loss: 2.8983829021453857\n",
      "Epoch 338/500, Train Loss: 2.8971686363220215\n",
      "Epoch 338/500, Val Loss: 2.898024082183838\n",
      "Epoch 339/500, Train Loss: 2.8973605632781982\n",
      "Epoch 339/500, Val Loss: 2.8982763290405273\n",
      "Epoch 340/500, Train Loss: 2.8965163230895996\n",
      "Epoch 340/500, Val Loss: 2.8981146812438965\n",
      "Epoch 341/500, Train Loss: 2.897066593170166\n",
      "Epoch 341/500, Val Loss: 2.8991615772247314\n",
      "Epoch 342/500, Train Loss: 2.896693229675293\n",
      "Epoch 342/500, Val Loss: 2.8998167514801025\n",
      "Epoch 343/500, Train Loss: 2.895850896835327\n",
      "Epoch 343/500, Val Loss: 2.896930694580078\n",
      "Epoch 344/500, Train Loss: 2.8957674503326416\n",
      "Epoch 344/500, Val Loss: 2.8964686393737793\n",
      "Epoch 345/500, Train Loss: 2.895688533782959\n",
      "Epoch 345/500, Val Loss: 2.8966853618621826\n",
      "Epoch 346/500, Train Loss: 2.894674301147461\n",
      "Epoch 346/500, Val Loss: 2.8973817825317383\n",
      "Epoch 347/500, Train Loss: 2.8951103687286377\n",
      "Epoch 347/500, Val Loss: 2.8954057693481445\n",
      "Epoch 348/500, Train Loss: 2.8945064544677734\n",
      "Epoch 348/500, Val Loss: 2.8963236808776855\n",
      "Epoch 349/500, Train Loss: 2.8940861225128174\n",
      "Epoch 349/500, Val Loss: 2.8968138694763184\n",
      "Epoch 350/500, Train Loss: 2.894331932067871\n",
      "Epoch 350/500, Val Loss: 2.896341323852539\n",
      "Epoch 351/500, Train Loss: 2.8932504653930664\n",
      "Epoch 351/500, Val Loss: 2.8959102630615234\n",
      "Epoch 352/500, Train Loss: 2.8947267532348633\n",
      "Epoch 352/500, Val Loss: 2.89382004737854\n",
      "Epoch 353/500, Train Loss: 2.8940811157226562\n",
      "Epoch 353/500, Val Loss: 2.8931636810302734\n",
      "Epoch 354/500, Train Loss: 2.892943859100342\n",
      "Epoch 354/500, Val Loss: 2.8953280448913574\n",
      "Epoch 355/500, Train Loss: 2.892737865447998\n",
      "Epoch 355/500, Val Loss: 2.8950538635253906\n",
      "Epoch 356/500, Train Loss: 2.8935232162475586\n",
      "Epoch 356/500, Val Loss: 2.894120931625366\n",
      "Epoch 357/500, Train Loss: 2.892212152481079\n",
      "Epoch 357/500, Val Loss: 2.894251585006714\n",
      "Epoch 358/500, Train Loss: 2.892160415649414\n",
      "Epoch 358/500, Val Loss: 2.893486976623535\n",
      "Epoch 359/500, Train Loss: 2.891629219055176\n",
      "Epoch 359/500, Val Loss: 2.892954111099243\n",
      "Epoch 360/500, Train Loss: 2.8916022777557373\n",
      "Epoch 360/500, Val Loss: 2.893627405166626\n",
      "Epoch 361/500, Train Loss: 2.8919730186462402\n",
      "Epoch 361/500, Val Loss: 2.893017530441284\n",
      "Epoch 362/500, Train Loss: 2.8912835121154785\n",
      "Epoch 362/500, Val Loss: 2.8919711112976074\n",
      "Epoch 363/500, Train Loss: 2.8908958435058594\n",
      "Epoch 363/500, Val Loss: 2.891936779022217\n",
      "Epoch 364/500, Train Loss: 2.8906679153442383\n",
      "Epoch 364/500, Val Loss: 2.8931570053100586\n",
      "Epoch 365/500, Train Loss: 2.8905961513519287\n",
      "Epoch 365/500, Val Loss: 2.8912153244018555\n",
      "Epoch 366/500, Train Loss: 2.889864683151245\n",
      "Epoch 366/500, Val Loss: 2.8916895389556885\n",
      "Epoch 367/500, Train Loss: 2.8900609016418457\n",
      "Epoch 367/500, Val Loss: 2.891756772994995\n",
      "Epoch 368/500, Train Loss: 2.889479637145996\n",
      "Epoch 368/500, Val Loss: 2.8917579650878906\n",
      "Epoch 369/500, Train Loss: 2.889570713043213\n",
      "Epoch 369/500, Val Loss: 2.890176296234131\n",
      "Epoch 370/500, Train Loss: 2.889892816543579\n",
      "Epoch 370/500, Val Loss: 2.8903164863586426\n",
      "Epoch 371/500, Train Loss: 2.889893054962158\n",
      "Epoch 371/500, Val Loss: 2.890272855758667\n",
      "Epoch 372/500, Train Loss: 2.888491630554199\n",
      "Epoch 372/500, Val Loss: 2.890448570251465\n",
      "Epoch 373/500, Train Loss: 2.8884105682373047\n",
      "Epoch 373/500, Val Loss: 2.8897223472595215\n",
      "Epoch 374/500, Train Loss: 2.8883142471313477\n",
      "Epoch 374/500, Val Loss: 2.890834331512451\n",
      "Epoch 375/500, Train Loss: 2.887272357940674\n",
      "Epoch 375/500, Val Loss: 2.889759063720703\n",
      "Epoch 376/500, Train Loss: 2.8876419067382812\n",
      "Epoch 376/500, Val Loss: 2.889163017272949\n",
      "Epoch 377/500, Train Loss: 2.8874337673187256\n",
      "Epoch 377/500, Val Loss: 2.889077663421631\n",
      "Epoch 378/500, Train Loss: 2.8873515129089355\n",
      "Epoch 378/500, Val Loss: 2.88969087600708\n",
      "Epoch 379/500, Train Loss: 2.8870913982391357\n",
      "Epoch 379/500, Val Loss: 2.8874459266662598\n",
      "Epoch 380/500, Train Loss: 2.8877010345458984\n",
      "Epoch 380/500, Val Loss: 2.888012647628784\n",
      "Epoch 381/500, Train Loss: 2.8868019580841064\n",
      "Epoch 381/500, Val Loss: 2.8877477645874023\n",
      "Epoch 382/500, Train Loss: 2.8863515853881836\n",
      "Epoch 382/500, Val Loss: 2.88783597946167\n",
      "Epoch 383/500, Train Loss: 2.8866214752197266\n",
      "Epoch 383/500, Val Loss: 2.8877830505371094\n",
      "Epoch 384/500, Train Loss: 2.8868532180786133\n",
      "Epoch 384/500, Val Loss: 2.8879446983337402\n",
      "Epoch 385/500, Train Loss: 2.8859262466430664\n",
      "Epoch 385/500, Val Loss: 2.8887693881988525\n",
      "Epoch 386/500, Train Loss: 2.885425329208374\n",
      "Epoch 386/500, Val Loss: 2.887173652648926\n",
      "Epoch 387/500, Train Loss: 2.8851370811462402\n",
      "Epoch 387/500, Val Loss: 2.8867053985595703\n",
      "Epoch 388/500, Train Loss: 2.8858954906463623\n",
      "Epoch 388/500, Val Loss: 2.8877499103546143\n",
      "Epoch 389/500, Train Loss: 2.884827136993408\n",
      "Epoch 389/500, Val Loss: 2.8873937129974365\n",
      "Epoch 390/500, Train Loss: 2.884812355041504\n",
      "Epoch 390/500, Val Loss: 2.8861703872680664\n",
      "Epoch 391/500, Train Loss: 2.8857924938201904\n",
      "Epoch 391/500, Val Loss: 2.885995864868164\n",
      "Epoch 392/500, Train Loss: 2.885057210922241\n",
      "Epoch 392/500, Val Loss: 2.886234760284424\n",
      "Epoch 393/500, Train Loss: 2.884906768798828\n",
      "Epoch 393/500, Val Loss: 2.8851754665374756\n",
      "Epoch 394/500, Train Loss: 2.885690212249756\n",
      "Epoch 394/500, Val Loss: 2.8886971473693848\n",
      "Epoch 395/500, Train Loss: 2.8871216773986816\n",
      "Epoch 395/500, Val Loss: 2.887152671813965\n",
      "Epoch 396/500, Train Loss: 2.8849477767944336\n",
      "Epoch 396/500, Val Loss: 2.8850972652435303\n",
      "Epoch 397/500, Train Loss: 2.8848464488983154\n",
      "Epoch 397/500, Val Loss: 2.885241985321045\n",
      "Epoch 398/500, Train Loss: 2.8833823204040527\n",
      "Epoch 398/500, Val Loss: 2.8860912322998047\n",
      "Epoch 399/500, Train Loss: 2.883769989013672\n",
      "Epoch 399/500, Val Loss: 2.885251998901367\n",
      "Epoch 400/500, Train Loss: 2.8853392601013184\n",
      "Epoch 400/500, Val Loss: 2.8859729766845703\n",
      "Epoch 401/500, Train Loss: 2.8847384452819824\n",
      "Epoch 401/500, Val Loss: 2.8871893882751465\n",
      "Epoch 402/500, Train Loss: 2.8855528831481934\n",
      "Epoch 402/500, Val Loss: 2.8839688301086426\n",
      "Epoch 403/500, Train Loss: 2.88277006149292\n",
      "Epoch 403/500, Val Loss: 2.88649582862854\n",
      "Epoch 404/500, Train Loss: 2.8841121196746826\n",
      "Epoch 404/500, Val Loss: 2.8907108306884766\n",
      "Epoch 405/500, Train Loss: 2.8904647827148438\n",
      "Epoch 405/500, Val Loss: 2.8836326599121094\n",
      "Epoch 406/500, Train Loss: 2.881594181060791\n",
      "Epoch 406/500, Val Loss: 2.8926939964294434\n",
      "Epoch 407/500, Train Loss: 2.8912901878356934\n",
      "Epoch 407/500, Val Loss: 2.914940357208252\n",
      "Epoch 408/500, Train Loss: 2.9133057594299316\n",
      "Epoch 408/500, Val Loss: 2.893881320953369\n",
      "Epoch 409/500, Train Loss: 2.893601417541504\n",
      "Epoch 409/500, Val Loss: 2.9094479084014893\n",
      "Epoch 410/500, Train Loss: 2.908132553100586\n",
      "Epoch 410/500, Val Loss: 2.888037919998169\n",
      "Epoch 411/500, Train Loss: 2.886443614959717\n",
      "Epoch 411/500, Val Loss: 2.900437355041504\n",
      "Epoch 412/500, Train Loss: 2.898817539215088\n",
      "Epoch 412/500, Val Loss: 2.8839967250823975\n",
      "Detener entrenamiento en la época 411, la mejor pérdida fue 2.88363\n"
     ]
    }
   ],
   "source": [
    "# Configuración\n",
    "from utils import EarlyStopping\n",
    "\n",
    "\n",
    "MAX_DIM = 20\n",
    "input_dim = 2\n",
    "hidden_dim = 30\n",
    "n_layers = 1\n",
    "dropout = 0\n",
    "epochs = 500\n",
    "teacher_forcing_ratio = 1.0  # Iniciamos con teacher forcing completo\n",
    "teacher_forcing_decay = 0.95  # Decae 5% por época\n",
    "\n",
    "# Modelo\n",
    "model = AutoRegressiveBinPackingModel(input_dim, hidden_dim, MAX_DIM, n_layers, dropout)\n",
    "model.to(DEVICE)\n",
    "# Hiperparámetros\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "teacher_forcing_ratio = 0.5  # 50% de probabilidad de usar teacher forcing\n",
    "\n",
    "def evaluate(model, criterion, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en los datos proporcionados y calcula la pérdida promedio.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): El modelo que se va a evaluar.\n",
    "        criterion (torch.nn.Module): La función de pérdida que se utilizará para calcular la pérdida.\n",
    "        data_loader (torch.utils.data.DataLoader): DataLoader que proporciona los datos de evaluación.\n",
    "\n",
    "    Returns:\n",
    "        float: La pérdida promedio en el conjunto de datos de evaluación.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()  # ponemos el modelo en modo de evaluacion\n",
    "    total_loss = 0  # acumulador de la perdida\n",
    "    with torch.no_grad():  # deshabilitamos el calculo de gradientes\n",
    "        for container, target_seq in data_loader:  # iteramos sobre el dataloader\n",
    "            container = container.to(device)  # movemos los datos al dispositivo\n",
    "            target_seq = target_seq.to(device)  # movemos los datos al dispositivo\n",
    "            logits_width, logits_height = model(\n",
    "                container, \n",
    "                target_seq=target_seq, \n",
    "                seq_len=target_seq.size(1), \n",
    "                teacher_forcing_ratio=teacher_forcing_ratio\n",
    "            )\n",
    "            target_width = target_seq[:, :, 0].long()  # (batch_size, seq_len)\n",
    "            target_height = target_seq[:, :, 1].long()  # (batch_size, seq_len)\n",
    "            loss_width = criterion(logits_width.view(-1, MAX_DIM+1), target_width.view(-1))\n",
    "            loss_height = criterion(logits_height.view(-1, MAX_DIM+1), target_height.view(-1))\n",
    "            loss = (loss_width + loss_height).item()\n",
    "            total_loss += loss  # acumulamos la perdida\n",
    "    return total_loss / len(data_loader)  # retornamos la perdida promedio\n",
    "\n",
    "early_stopping = EarlyStopping(patience=7)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    teacher_forcing_ratio *= teacher_forcing_decay  # Reducimos el ratio por cada época\n",
    "    train_loss = 0\n",
    "    \n",
    "    for container, target_seq in train_dataloader:\n",
    "        container = container.to(DEVICE)\n",
    "        target_seq = target_seq.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits_width, logits_height = model(\n",
    "            container, \n",
    "            target_seq=target_seq, \n",
    "            seq_len=target_seq.size(1), \n",
    "            teacher_forcing_ratio=teacher_forcing_ratio\n",
    "        )\n",
    "        \n",
    "        # Aplanar las salidas para calcular la pérdida\n",
    "        target_width = target_seq[:, :, 0].long()  # (batch_size, seq_len)\n",
    "        target_height = target_seq[:, :, 1].long()  # (batch_size, seq_len)\n",
    "        loss_width = criterion(logits_width.view(-1, MAX_DIM+1), target_width.view(-1))\n",
    "        loss_height = criterion(logits_height.view(-1, MAX_DIM+1), target_height.view(-1))\n",
    "        \n",
    "        loss = loss_width + loss_height\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_dataloader)}\")\n",
    "    val_loss = evaluate(model, criterion, val_dataloader, DEVICE)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {val_loss}\")\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Detener entrenamiento en la época {epoch}, la mejor pérdida fue {early_stopping.best_score:.5f}\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BinPackingGame, Box, ResolvedBinPackingGameResult\n",
    "dataset_keys = set()\n",
    "\n",
    "def tensor_to_box(tensor):\n",
    "    return Box(int(tensor[0].item()), int(tensor[1].item()))\n",
    "    \n",
    "\n",
    "for container_tensor, boxes_tensor in train_dataset:\n",
    "\n",
    "    boxes = [tensor_to_box(tensor) for tensor in boxes_tensor]\n",
    "\n",
    "    game = BinPackingGame(tensor_to_box(container_tensor), boxes)\n",
    "    dataset_keys.add(hash(game))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container.shape=torch.Size([1, 2])\n",
      "Secuencia generada:\n",
      "Ancho: 1, Alto: 3\n",
      "Ancho: 10, Alto: 1\n",
      "Ancho: 3, Alto: 5\n",
      "Ancho: 7, Alto: 1\n",
      "Ancho: 1, Alto: 6\n",
      "Ancho: 1, Alto: 1\n",
      "Ancho: 1, Alto: 4\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(model, container, max_seq_len=10, teacher_forcing_ratio=0.0):\n",
    "    \"\"\"\n",
    "    Genera una secuencia de cajas para un contenedor dado utilizando un modelo entrenado.\n",
    "    \n",
    "    Args:\n",
    "        model: El modelo entrenado.\n",
    "        container: Tensor con las dimensiones del contenedor (batch_size, input_dim).\n",
    "        max_seq_len: Longitud máxima de la secuencia que se desea generar.\n",
    "        teacher_forcing_ratio: Probabilidad de usar teacher forcing (aunque normalmente se usa 0.0 aquí).\n",
    "        \n",
    "    Returns:\n",
    "        Secuencia generada de dimensiones (seq_len, 2), con el formato de (width, height).\n",
    "    \"\"\"\n",
    "    model.eval()  # Poner el modelo en modo evaluación\n",
    "    \n",
    "    # Verifica que el contenedor sea bidimensional\n",
    "    if container.ndim == 1:\n",
    "        container = container.unsqueeze(0)  # Convertir a (1, input_dim)\n",
    "    container = container.to(DEVICE)  # (batch_size, input_dim)\n",
    "    \n",
    "    # Embedding inicial del contenedor\n",
    "    container_emb = model.embedding(container).unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "    # Inicializa la secuencia generada y el estado oculto del LSTM\n",
    "    generated_seq = container_emb\n",
    "    hidden = None\n",
    "    generated_boxes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_seq_len):\n",
    "            # Paso del LSTM\n",
    "            output, hidden = model.lstm(generated_seq, hidden)  # (batch_size, seq_len, hidden_dim)\n",
    "            \n",
    "            # Logits para predicciones de width y height\n",
    "            logits_width = model.fc_width(output[:, -1, :])  # (batch_size, max_dim+1)\n",
    "            logits_height = model.fc_height(output[:, -1, :])  # (batch_size, max_dim+1)\n",
    "            \n",
    "            # Predicciones\n",
    "            prob_width = F.softmax(logits_width, dim=-1)\n",
    "            prob_height = F.softmax(logits_height, dim=-1)\n",
    "            next_width = torch.multinomial(prob_width, num_samples=1).squeeze(-1)  # (batch_size,)\n",
    "            next_height = torch.multinomial(prob_height, num_samples=1).squeeze(-1)  # (batch_size,)\n",
    "\n",
    "            # Construir la siguiente caja\n",
    "            next_box = torch.stack([next_width, next_height], dim=1)  # (batch_size, 2)\n",
    "\n",
    "            #Si el ancho o el alto es 0 se termina la secuencia\n",
    "            if next_box[0][0] == 0 or next_box[0][1] == 0:\n",
    "                break\n",
    "\n",
    "            generated_boxes.append(next_box.cpu().numpy())  # Guardar la predicción\n",
    "            \n",
    "            # Embedding de la siguiente caja\n",
    "            next_box_emb = model.embedding(next_box.float().to(DEVICE)).unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "            # Actualiza la secuencia generada agregando la nueva caja\n",
    "            generated_seq = torch.cat([generated_seq, next_box_emb], dim=1)\n",
    "\n",
    "    return generated_boxes\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "container = torch.tensor([[10, 10]], dtype=torch.float32)  # Ejemplo de contenedor (width=10, height=10)\n",
    "print(f\"{container.shape=}\")\n",
    "generated_seq = generate_sequence(model, container, max_seq_len=10, teacher_forcing_ratio=0.0)\n",
    "\n",
    "# Mostrar la secuencia generada\n",
    "print(\"Secuencia generada:\")\n",
    "for box in generated_seq:\n",
    "    print(f\"Ancho: {box[0][0]}, Alto: {box[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid games: 16/50\n",
      "Unique games: 16/50\n",
      "unique_games keys: {1904146084869635521, 4604776546048649505, 7077992645349355590, 1204996520032874761, -3904023141944031124, 2872195147997666122, -5927144692391268660, 8521919048154952460, 4804942357064857743, 2055797294419686002, -6317749523471701483, 1918916807093795958, 579696818835013047, 7161926823480877142, -803580345184116294, 8392948918768709368}\n",
      "Coverages: {0.3375, 0.825, 0.925, 0.5125, 0.775, 0.75, 0.8, 0.875, 0.6375, 0.375, 0.65, 0.9625, 0.1125, 0.6}\n",
      "New games: 16\n",
      "Boxes count: Counter({5: 7, 2: 3, 6: 3, 3: 2, 7: 1})\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=4, height=5), Box(width=7, height=2), Box(width=8, height=3), Box(width=4, height=3), Box(width=4, height=4)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=2, height=1), Box(width=6, height=9), Box(width=6, height=4), Box(width=2, height=6), Box(width=15, height=2), Box(width=2, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=8, height=7), Box(width=1, height=5), Box(width=3, height=4), Box(width=1, height=1), Box(width=2, height=2)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=4, height=1), Box(width=9, height=10), Box(width=4, height=1), Box(width=3, height=6), Box(width=1, height=5)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=13, height=1), Box(width=8, height=4), Box(width=4, height=3), Box(width=2, height=1), Box(width=1, height=5)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=4, height=13), Box(width=2, height=3), Box(width=1, height=3), Box(width=7, height=4), Box(width=1, height=6)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=3, height=4), Box(width=1, height=6), Box(width=1, height=5), Box(width=3, height=1), Box(width=1, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=10, height=6), Box(width=5, height=7), Box(width=1, height=3), Box(width=2, height=5), Box(width=9, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=6, height=2), Box(width=1, height=10), Box(width=1, height=1), Box(width=6, height=4), Box(width=3, height=4), Box(width=1, height=1), Box(width=6, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=7, height=11), Box(width=4, height=7), Box(width=3, height=2), Box(width=2, height=7), Box(width=1, height=6), Box(width=6, height=4)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=1, height=20), Box(width=11, height=1), Box(width=2, height=10), Box(width=3, height=7), Box(width=4, height=1), Box(width=3, height=4)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=8, height=3), Box(width=3, height=4), Box(width=4, height=3), Box(width=6, height=1), Box(width=8, height=6)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=3, height=5), Box(width=7, height=4), Box(width=4, height=2), Box(width=1, height=6), Box(width=1, height=2), Box(width=4, height=6)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=4, height=6), Box(width=5, height=4), Box(width=1, height=1), Box(width=3, height=8), Box(width=5, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=6, height=5), Box(width=4, height=8)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=6, height=10), Box(width=10, height=3), Box(width=1, height=7), Box(width=1, height=1), Box(width=2, height=2), Box(width=1, height=1), Box(width=4, height=2)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=3, height=11), Box(width=1, height=6), Box(width=5, height=6), Box(width=4, height=5), Box(width=10, height=2), Box(width=1, height=6)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=2, height=10), Box(width=2, height=1), Box(width=1, height=1), Box(width=2, height=5), Box(width=1, height=6), Box(width=2, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=8, height=7), Box(width=7, height=3), Box(width=5, height=1), Box(width=4, height=4), Box(width=1, height=2), Box(width=1, height=6)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=6, height=10), Box(width=2, height=6), Box(width=7, height=3), Box(width=5, height=12), Box(width=2, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=4, height=7), Box(width=6, height=3), Box(width=4, height=1), Box(width=3, height=14), Box(width=4, height=1), Box(width=7, height=1), Box(width=2, height=6)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=1, height=7), Box(width=4, height=9), Box(width=7, height=1), Box(width=1, height=6), Box(width=1, height=1), Box(width=3, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=2, height=10), Box(width=3, height=8), Box(width=3, height=1), Box(width=6, height=2), Box(width=5, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=5, height=2), Box(width=5, height=7), Box(width=3, height=5), Box(width=4, height=6), Box(width=4, height=2)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=7, height=3), Box(width=5, height=8), Box(width=1, height=1), Box(width=1, height=3), Box(width=5, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=10, height=3), Box(width=7, height=5), Box(width=18, height=7), Box(width=1, height=6), Box(width=2, height=9), Box(width=3, height=3)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=7, height=1), Box(width=5, height=4), Box(width=14, height=10), Box(width=1, height=6), Box(width=3, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=10, height=13), Box(width=3, height=7), Box(width=7, height=3), Box(width=8, height=6), Box(width=1, height=8)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=11, height=4), Box(width=5, height=1), Box(width=12, height=5), Box(width=5, height=2)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=9, height=4), Box(width=2, height=1), Box(width=4, height=2), Box(width=1, height=3), Box(width=1, height=1), Box(width=1, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=4, height=9), Box(width=2, height=6), Box(width=9, height=6), Box(width=5, height=3), Box(width=1, height=5)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=9, height=6), Box(width=5, height=7), Box(width=11, height=3), Box(width=4, height=1), Box(width=4, height=1), Box(width=6, height=7)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=9, height=2), Box(width=6, height=3), Box(width=1, height=7), Box(width=8, height=3), Box(width=2, height=2), Box(width=3, height=6)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=2, height=8), Box(width=2, height=7)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=8, height=1), Box(width=1, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=4, height=5), Box(width=10, height=3), Box(width=2, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=5, height=12), Box(width=3, height=4), Box(width=2, height=3), Box(width=2, height=3), Box(width=1, height=3)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=3, height=3), Box(width=2, height=2), Box(width=2, height=6), Box(width=5, height=3), Box(width=2, height=4)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=2, height=4), Box(width=12, height=5), Box(width=8, height=6), Box(width=2, height=4), Box(width=1, height=2), Box(width=5, height=8)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=7, height=5), Box(width=9, height=3), Box(width=9, height=6), Box(width=7, height=1), Box(width=8, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=6, height=2), Box(width=1, height=1), Box(width=3, height=6), Box(width=4, height=12), Box(width=1, height=12), Box(width=2, height=10)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=2, height=2), Box(width=2, height=1), Box(width=6, height=3), Box(width=4, height=10), Box(width=4, height=9)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=7, height=1), Box(width=1, height=5), Box(width=5, height=6), Box(width=2, height=1), Box(width=2, height=4)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=2, height=3), Box(width=2, height=7), Box(width=2, height=1), Box(width=2, height=2), Box(width=1, height=1)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=9, height=14)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=3, height=2), Box(width=12, height=3), Box(width=8, height=1), Box(width=8, height=4), Box(width=1, height=10)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=1, height=1), Box(width=1, height=2), Box(width=1, height=7), Box(width=11, height=1), Box(width=2, height=4), Box(width=1, height=3)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=1, height=11), Box(width=9, height=5), Box(width=3, height=7), Box(width=8, height=1), Box(width=1, height=5)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=6, height=6), Box(width=7, height=3), Box(width=5, height=5), Box(width=5, height=1), Box(width=5, height=4), Box(width=6, height=3)])\n",
      "BinPackingGame(container=Box(width=10, height=8), boxes=[Box(width=10, height=3), Box(width=3, height=4), Box(width=7, height=5)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Counter\n",
    "\n",
    "from models import BinPackingGame, Box\n",
    "\n",
    "\n",
    "# Generación de cajas\n",
    "container_width = 10\n",
    "container_height = 8\n",
    "\n",
    "attempts = 50\n",
    "valid_games = 0\n",
    "games = []\n",
    "unique_games = set()\n",
    "coverages = set()\n",
    "boxes_count = Counter()\n",
    "new_games = 0\n",
    "model.eval()\n",
    "for i in range(attempts):\n",
    "    container = torch.tensor([[container_width, container_height]],dtype=torch.float32).to(DEVICE)\n",
    "    generated_boxes = generate_sequence(model, container, max_seq_len=10, teacher_forcing_ratio=0.0)\n",
    "\n",
    "    # Paso 2: Convertir a lista de tuplas\n",
    "    # box_list = [tensor_to_box(tensor) for tensor in output]\n",
    "\n",
    "    boxes = [Box(int(gen_box[0][0]), int(gen_box[0][1])) for gen_box in generated_boxes]\n",
    "\n",
    "    valid_boxes = [box for box in boxes if box.width > 0 and box.height > 0]\n",
    "    if len(valid_boxes) == 0:\n",
    "        continue\n",
    "    game = BinPackingGame(Box(container_width, container_height), valid_boxes)\n",
    "    games.append(game)\n",
    "    result = game.solve()\n",
    "    if isinstance(result, ResolvedBinPackingGameResult):\n",
    "        valid_games += 1\n",
    "        game_key = hash(game)\n",
    "        boxes_count[len(game.boxes)] += 1\n",
    "        # print(f'{game_key=}')\n",
    "        if game_key not in unique_games:\n",
    "            unique_games.add(game_key)\n",
    "            coverages.add(game.coverage())\n",
    "            if game_key not in dataset_keys:\n",
    "                new_games += 1\n",
    "\n",
    "print(f\"Valid games: {valid_games}/{attempts}\")\n",
    "print(f\"Unique games: {len(unique_games)}/{attempts}\")\n",
    "print(f\"unique_games keys: {unique_games}\")\n",
    "print(f\"Coverages: {coverages}\")\n",
    "print(f\"New games: {new_games}\")\n",
    "print(f\"Boxes count: {boxes_count}\")\n",
    "[print(f'{game}') for game in games]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
