{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import train\n",
    "from bin_packing_dataset import BinPackingDataset\n",
    "from bin_packing_model import BinPackingLSTMModel\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 23\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Num Workers: 0\n"
     ]
    }
   ],
   "source": [
    "# Algunas constantes\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "\n",
    "NUM_WORKERS = 0 # max(os.cpu_count() - 1, 1)  # número de workers para cargar los datos\n",
    "\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Num Workers: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 81000\n",
      "Container: tensor([14., 11.])\n",
      "Boxes: tensor([[ 8.,  7.],\n",
      "        [12.,  2.],\n",
      "        [ 1.,  2.],\n",
      "        [ 2.,  7.],\n",
      "        [ 4.,  8.]])\n",
      "Train dataset size: 56700\n",
      "Val dataset size: 16200\n",
      "Test dataset size: 8100\n"
     ]
    }
   ],
   "source": [
    "#Creacion del dataset de entrenamiento, validacion y test\n",
    "\n",
    "full_dataset = BinPackingDataset('data')\n",
    "print('Full dataset size:', len(full_dataset))\n",
    "container_tensor, boxes_tensor = full_dataset[0]\n",
    "print('Container:', container_tensor)\n",
    "print('Boxes:', boxes_tensor)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [int(0.7*len(full_dataset)), int(0.20*len(full_dataset)), int(0.10*len(full_dataset))])\n",
    "print('Train dataset size:', len(train_dataset))\n",
    "print('Val dataset size:', len(val_dataset))\n",
    "print('Test dataset size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del primer contenedor: tensor([14., 11.])\n",
      "Tamaño de las cajas del primer contenedor: tensor([[ 8.,  7.],\n",
      "        [12.,  2.],\n",
      "        [ 1.,  2.],\n",
      "        [ 2.,  7.],\n",
      "        [ 4.,  8.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "# Collate para manejar secuencias de diferentes longitudes\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "def custom_collate_fn_with_padding(batch):\n",
    "    \"\"\"\n",
    "    Collate function que mantiene la estructura de contenedor y agrega padding a las secuencias de cajas.\n",
    "    \n",
    "    Args:\n",
    "        batch (list): Lista de tuplas (contenedor, cajas).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (contenedores, cajas_padded, longitudes) donde:\n",
    "            - contenedores: Tensor de tamaño (batch_size, 2).\n",
    "            - cajas_padded: Tensor de tamaño (batch_size, max_len, 2) con padding.\n",
    "            - longitudes: Tensor de tamaños originales de las secuencias de cajas.\n",
    "    \"\"\"\n",
    "    containers = torch.stack([item[0] for item in batch])  # Contenedores como tensor\n",
    "    boxes = [item[1] for item in batch]  # Lista de cajas\n",
    "    \n",
    "    # Padding de las secuencias de cajas (rellenar con ceros hasta la longitud máxima en el batch)\n",
    "    boxes_padded = rnn_utils.pad_sequence(boxes, batch_first=True)\n",
    "    \n",
    "    # Longitudes originales de cada secuencia de cajas\n",
    "    lengths = torch.tensor([len(b) for b in boxes])\n",
    "    \n",
    "    return containers, boxes_padded\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "mock_loader = torch.utils.data.DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_with_padding)\n",
    "\n",
    "x, y = next(iter(mock_loader))\n",
    "print('Tamaño del primer contenedor:', x[0])\n",
    "print('Tamaño de las cajas del primer contenedor:', y[0])\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn_with_padding)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn_with_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer modelo: Seq2Seq\n",
    "\n",
    "La idea inicial es usar un Seq2Seq que pueda generar secuencias de cajas a partir del tamaño del contenedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_m1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002 | Train Loss: 5.28223 | Val Loss: 5.22082\n",
      "Epoch: 004 | Train Loss: 5.21119 | Val Loss: 5.18844\n",
      "Epoch: 006 | Train Loss: 5.21283 | Val Loss: 5.18860\n",
      "Epoch: 008 | Train Loss: 5.17334 | Val Loss: 5.16877\n",
      "Epoch: 010 | Train Loss: 5.18539 | Val Loss: 5.17353\n",
      "Epoch: 012 | Train Loss: 5.17877 | Val Loss: 5.16997\n",
      "Epoch: 014 | Train Loss: 5.18614 | Val Loss: 5.16890\n",
      "Epoch: 016 | Train Loss: 5.17091 | Val Loss: 5.16022\n",
      "Epoch: 018 | Train Loss: 5.19487 | Val Loss: 5.19058\n",
      "Epoch: 020 | Train Loss: 5.17606 | Val Loss: 5.16174\n",
      "Epoch: 022 | Train Loss: 5.16321 | Val Loss: 5.19332\n",
      "Epoch: 024 | Train Loss: 5.17122 | Val Loss: 5.16046\n",
      "Epoch: 026 | Train Loss: 5.17440 | Val Loss: 5.19140\n",
      "Epoch: 028 | Train Loss: 5.16787 | Val Loss: 5.16237\n",
      "Epoch: 030 | Train Loss: 5.16811 | Val Loss: 5.15758\n",
      "Epoch: 032 | Train Loss: 5.17813 | Val Loss: 5.16229\n",
      "Epoch: 034 | Train Loss: 5.17654 | Val Loss: 5.17877\n",
      "Epoch: 036 | Train Loss: 5.19115 | Val Loss: 5.19917\n",
      "Epoch: 038 | Train Loss: 5.18211 | Val Loss: 5.18046\n",
      "Epoch: 040 | Train Loss: 5.18056 | Val Loss: 5.17157\n",
      "Epoch: 042 | Train Loss: 5.17191 | Val Loss: 5.14972\n",
      "Epoch: 044 | Train Loss: 5.16740 | Val Loss: 5.15969\n",
      "Epoch: 046 | Train Loss: 5.18708 | Val Loss: 5.16745\n",
      "Epoch: 048 | Train Loss: 5.18654 | Val Loss: 5.18508\n",
      "Epoch: 050 | Train Loss: 5.17056 | Val Loss: 5.16121\n"
     ]
    }
   ],
   "source": [
    "from bin_packing_seq_2_seq_solver import BinPackingSeq2SeqSolver\n",
    "from utils import print_log\n",
    "\n",
    "seq2seq = BinPackingSeq2SeqSolver(train_loader=train_dataloader, val_loader=val_dataloader, log_fn=print_log, device=DEVICE)\n",
    "\n",
    "model, epoch_train_errors, epoch_val_errors = seq2seq.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BinPackingGame, Box, ResolvedBinPackingGameResult\n",
    "dataset_keys = set()\n",
    "\n",
    "def tensor_to_box(tensor):\n",
    "    return Box(int(tensor[0].item()), int(tensor[1].item()))\n",
    "    \n",
    "\n",
    "for container_tensor, boxes_tensor in test_dataset:\n",
    "\n",
    "    boxes = [tensor_to_box(tensor) for tensor in boxes_tensor]\n",
    "\n",
    "    game = BinPackingGame(tensor_to_box(container_tensor), boxes)\n",
    "    game_key = game.generate_unique_key()\n",
    "    dataset_keys.add(game_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid games: 100/100\n",
      "Unique games: 1/100\n",
      "unique_games keys: {'4895df429991ff1f9e6f4bb161b49298'}\n",
      "Coverages: {0.7428571428571429}\n",
      "New games: 1\n",
      "Boxes count: Counter({5: 100})\n"
     ]
    }
   ],
   "source": [
    "from typing import Counter\n",
    "from models import BinPackingGame, Box, ResolvedBinPackingGameResult\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "container_width = 7\n",
    "container_height = 5\n",
    "\n",
    "attempts = 100\n",
    "valid_games = 0\n",
    "unique_games = set()\n",
    "coverages = set()\n",
    "boxes_count = Counter()\n",
    "new_games = 0\n",
    "for i in range(attempts):\n",
    "    input = torch.tensor([[container_width, container_height]],dtype=torch.float32).to(DEVICE)\n",
    "    output = model(input, 100)\n",
    "\n",
    "    filtered_tensor = output[output != 0].view(-1, 2)\n",
    "\n",
    "    # Paso 2: Convertir a lista de tuplas\n",
    "    box_list = [tensor_to_box(tensor) for tensor in filtered_tensor]\n",
    "\n",
    "    # boxes = [Box(int(box_width), int(box_height)) for(box_width, box_height) in box_list]\n",
    "\n",
    "    valid_boxes = [box for box in box_list if box.width > 0 and box.height > 0]\n",
    "\n",
    "    game = BinPackingGame(Box(container_width, container_height), valid_boxes)\n",
    "    result = game.solve()\n",
    "    if isinstance(result, ResolvedBinPackingGameResult):\n",
    "        valid_games += 1\n",
    "        game_key = game.generate_unique_key()\n",
    "        boxes_count[len(game.boxes)] += 1\n",
    "        # print(f'{game_key=}')\n",
    "        if game_key not in unique_games:\n",
    "            unique_games.add(game_key)\n",
    "            coverages.add(game.coverage())\n",
    "            if game_key not in dataset_keys:\n",
    "                new_games += 1\n",
    "\n",
    "print(f\"Valid games: {valid_games}/{attempts}\")\n",
    "print(f\"Unique games: {len(unique_games)}/{attempts}\")\n",
    "print(f\"unique_games keys: {unique_games}\")\n",
    "print(f\"Coverages: {coverages}\")\n",
    "print(f\"New games: {new_games}\")\n",
    "print(f\"Boxes count: {boxes_count}\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
